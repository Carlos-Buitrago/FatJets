
%NOTE: to be used with \usepackage{subfiles} in the main file.
%Subfiles go in folders which live with the main file.
%Bibliography and preamble go in the main file.

%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%
\providecommand{\main}{..}

\documentclass[main]{subfiles} %Each instance of `../' elevates one folder to find the main file

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%

% \tableofcontents % Can be useful to load a TOC while writing

\doublespacing

\schapter{Sample generation and machine learning techniques}

\hypsection{Generation of signal and background events}
\vspace{20pt}

The Monte Carlo samples used in the analysis are produced using a generation chain involving multiple steps involving different software. Both signal and background event generation consist of the same steps. Different samples are produced separately in order to obtain the respective signal and background datasets. Two signal processes will be considered, the first one is used to study the tagging of boosted top quarks and the second one to study the tagging of boosted $W$ bosons. These signal samples will be used together with a single QCD background sample of light quarks and gluons, taking into account the discussion of section \ref{sect:substructure}.\\


The generation chain begins with the generation of matrix elements at leading order (LO) in QCD and electroweak couplings using \textsc{MadGraph5\_aMC@NLO} v2.9.12 \cite{Alwall2014} with the \textsc{NN23LO1} PDF set \cite{Ball2017}. The process defined to produce the signal top jets consists of top quarks decaying in the entirely hadronic channel $pp \rightarrow t \tilde{t}$, ($t \rightarrow W^+ b$, $W^+ \rightarrow jj$), ($\tilde{t} \rightarrow W^- \tilde{b}$, $W^- \rightarrow jj$). On the other hand, the process used to produce the background sample is the default QCD production dijet production routine $pp \rightarrow jj$ where the $j$ jets are composed of light quarks ($b$'s included) and gluons.\\

After the amplitudes and mappings for the relevant processes have been obtained they are passed to \textsc{MadEvent} \cite{Maltoni2003}, which generates the unweighted tree-level Les Houches events. Both signal and background datasets consist of 500000 events generated at a centre-of-mass energy of 14\;TeV. During the event generation stage, a parton-level momentum phase space cut is implemented on all partons in order to reduce computing time for the simulations. Only boosted particles are of interest for this study, and the parton-level cut allows more fat jets to be produced in the established number of events. The cut used is $p_T > 350$\;GeV and it applies at parton-level to both gluons and light quarks, as well as $W$ bosons and top quarks. \textbf{This initial $p_T$ cut value is later increased for subsequent analyses.}\\

The tree-level events are showered using \textsc{Pythia} v8.306 \cite{Sjostrand2015} with the default settings proposed in the \textsc{MadGraph5}'s implementation of \textsc{Pythia 8} in order to obtain hadron-level events in \textsc{HEPMC} format \cite{Dobbs2001}. Subsequently, the detector response is simulated via \textsc{Delphes} v3.5.0 \cite{deFavereau2014} using the default ATLAS detector configuration card with the inclusion of the fat jet reconstruction module provided by the \textsc{FastJet} v3.3.4 library \cite{Cacciari2012}.\\

The fat jets used in the analysis are reconstructed via \textsc{FastJet} using the anti-$k_T$ algorithm with $R =$ 1.0, \textbf{(this radius parameter is later increased for subsequent analyses)}. The $\tau_N$ values associated to each fat jet are computed from the three-momenta of $N$ candidate subjets using the $N$-subjettiness package (v2.2.6) of the \textsc{FastJet} contrib v1.051. In order to obtain the candidate subjets needed for the $\tau_N$ computations the fat jet is reclustered with an exclusive $k_T$ algorithm \cite{Khelifa-Kerfa2022}.\\

At high luminosity hadron colliders such as the LHC, the presence of pile-up energy and the underlying event interferes in analyses involving large $R$ jets because they lead to soft, wide-angle contaminations which lead to the dilution of the jet substructure. Grooming techniques are useful to remove these contaminations. The grooming methods applied to the jets reconstructed in this analysis are trimming \cite{Krohn2010}, pruning \cite{Ellis2009} and soft-dropping \cite{Larkoski2014} algorithms. These methods are implemented via \textsc{FastJet} integration, using the default parameters in the ATLAS configuration card of \textsc{Delphes 3}. This selection of parameters is done based on various studies that have been carried out previously in order to compare and optimize the performance of the different methods.\\

It should be noted that the description of the event generation process described above also applies for the generation of the signal sample used to study the tagging of boosted $W$ bosons. In this case the process used for the tree-level event generation is $W$ pair production, with each $W$ decaying in the full hadronic channel $pp \rightarrow W^{+}W^{-}$, ($W^{+} \rightarrow jj$), ($W^{-} \rightarrow jj$), where the $j$ jets are defined as in the process used to produce the background sample.\\


\hypsection{Machine learning implementation}
\label{sect:event-generation-chain}
\vspace{20pt}

The discrimination between signal and background events can be initially done as a cut based analysis by applying subsequent cuts on the substructure variables proposed earlier. This can be easily seen from the comparison between signal and background distributions of the jet mass and $\tau_{N+1}/\tau_N$ variables (see section \ref{sect:results}). However, in previous studies this kind of cut based selections have been found to be insufficient when dealing with data similar to that of this work \cite{Li2016}. With that in mind, the tagging method proposed in this study will be based on machine learning techniques, which take an increasingly important role in LHC experiments as time passes, specially in the field of jet tagging \cite{Cagnotta2022}.  \\

The machine learning aspect of the proposed tagging method consists on the use of multivariate analysis algorithms. Initially, multiple multivariate classifiers will be considered and their performance will be compared. The classifiers being evaluated are the k-nearest neighbour classifier (k-NN), linear discriminant analysis (LD), function discriminant analysis (FDA), 1-dimensional likelihood estimator (with and without PCA-transformed input variables), Friedman's RuleFit method (RuleFit), multilayer perceptron artificial neural network (MLP), boosted decision tree (BDT) and a deep learning neural network (DNN). For more information on each one of these methods see ref. \cite{Hoecker2009}.\\

The different multivariate methods are implemented using the \textsc{Toolkit for Multivariate Data Analysis with ROOT (\textsc{TMVA 4})} \cite{Hoecker2009}. This toolkit, integrated into the \textsc{ROOT} analysis framework \cite{Brun1997} allows for the training, testing, performance evaluation and application of the different multivariate classification algorithms. The multivariate techniques used belong to the family of "supervised learning" algorithms. They use a training set of events for which the desired output is known (i.e. the algorithm knows if the event is a signal or a background event) in order to determine the mapping function that describes the decision boundary used during the classification. \\

The discriminating variables used as input for the classifier algorithms are the fat jet mass and $N$-subjettiness, which contain information on the substructure of the fat jets. As it was mentioned before, the $\tau_N$ variables do not have much discriminant power by themselves, but rather their ratios do. An advantage of the multivariate classifiers is that they can be supplied with a full set of $\tau_N$ variables so that more complex relations between them can be taken into account when defining the decision boundary that optimizes the classification. As the number of $\tau_N$ variables used as input increases, so does the information that the algorithm can use to carry out the classification. However, it is expected that for higher $N$ values, $\tau_N$ won't contribute as much to the discrimination. Therefore the performance with different number of $\tau_N$ variables used as input will be considered. \\

The training and testing sets of events are randomly selected from the generated signal and background samples. For both signal and background samples, half of the events are assigned to the training dataset and the other half are assigned to the testing dataset so that they both have the same size. Lastly, it is worth mentioning that for all the multivariate methods, the default configuration of the classifier algorithms in \textsc{TMVA} is used. 




















% \bibliographystyle{../../PhilReview} %%bib style found in bst folder, in bibtex folder, in texmf folder.
% \nobibliography{Zotero} %%bib database found in bib folder, in bibtex folder
% \nobibliography{../../Thesis_bib}
\biblio

\end{document}
